{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0qJzN8zGrzZ"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "Q2wZg-M2GuBK",
        "outputId": "ee2638c0-06d4-493a-eba4-fa57cf0319bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mankitt6174\u001b[0m (\u001b[33mankit_6174\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "pGcWf4F5JcMc",
        "outputId": "f0e31558-2b86-4e9d-e90b-f7ffe9be6b38"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▂▂▃▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▃▃▄▅▅▄▆▆▇▆▆▆▇▆▇▇▆▄▇▁▇▅▇▆▇▅▆██▅▇▇▆▆██▇▆</td></tr><tr><td>val_loss</td><td>█▇▇▆▆█▄▄▃▄▃▃▃▃▂▃▂▃▂▆▃▂▃▄▁▂▁▁▁▂▃▅▃▂▂▂▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_acc</td><td>0.637</td></tr><tr><td>train_loss</td><td>0.90182</td></tr><tr><td>val_acc</td><td>0.5865</td></tr><tr><td>val_loss</td><td>1.0059</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Second_Run</strong> at: <a href='https://wandb.ai/ankit_6174/dna-mutation-predictor-50K/runs/yce5wdfi' target=\"_blank\">https://wandb.ai/ankit_6174/dna-mutation-predictor-50K/runs/yce5wdfi</a><br> View project at: <a href='https://wandb.ai/ankit_6174/dna-mutation-predictor-50K' target=\"_blank\">https://wandb.ai/ankit_6174/dna-mutation-predictor-50K</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250826_092053-yce5wdfi/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250826_093715-pos1rdwv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ankit_6174/dna-mutation-predictor-50K/runs/pos1rdwv' target=\"_blank\">Third_Run</a></strong> to <a href='https://wandb.ai/ankit_6174/dna-mutation-predictor-50K' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ankit_6174/dna-mutation-predictor-50K' target=\"_blank\">https://wandb.ai/ankit_6174/dna-mutation-predictor-50K</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ankit_6174/dna-mutation-predictor-50K/runs/pos1rdwv' target=\"_blank\">https://wandb.ai/ankit_6174/dna-mutation-predictor-50K/runs/pos1rdwv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ankit_6174/dna-mutation-predictor-50K/runs/pos1rdwv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x79e5bb462ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Define your hyperparameters\n",
        "hyperparameters = {\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"batch_size\": 128,\n",
        "    \"embed_dim\": 128,\n",
        "    \"num_heads\": 4,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.3,\n",
        "    \"ff_dim\": 1024,\n",
        "    \"dataset_size\": \"50K\",\n",
        "    \"epochs\": 50,\n",
        "    \"name\": \"Third_Run\",\n",
        "    \"Precision\": \"FP16\"\n",
        "}\n",
        "\n",
        "wandb.init(\n",
        "    project=\"dna-mutation-predictor-50K\",\n",
        "    config=hyperparameters,\n",
        "    name=hyperparameters['name']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH3j5rWdaQ4c",
        "outputId": "60243f79-9a50-4c46-98c4-0b67553f6673"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1v98WHCJoDO",
        "outputId": "c23e04fe-9fdb-4cfc-e7e3-348fd63190c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = f\"/content/drive/MyDrive/dataset/{hyperparameters['dataset_size']}.csv\"\n",
        "\n",
        "data = pd.read_csv(data_path)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wR3jf-oAJ2PE"
      },
      "outputs": [],
      "source": [
        "x = data['sequence']\n",
        "y = data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "j-XikXuLJ2M1"
      },
      "outputs": [],
      "source": [
        "def get_codon(seq, k=3):\n",
        "    return [seq[i:i+k] for i in range(len(seq) - k + 1)]\n",
        "\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "\n",
        "for seq in data['sequence']:\n",
        "    for codons in get_codon(seq.lower()):\n",
        "        if codons not in vocab:\n",
        "            vocab[codons] = len(vocab)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "def get_tensor(text):\n",
        "    return [vocab.get(codons.lower(), vocab['<UNK>']) for codons in get_codon(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3dzO0_F0J2Kw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x_frame = x\n",
        "    self.y_frame = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_frame)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = torch.tensor(get_tensor(self.x_frame[index]), dtype=torch.long)\n",
        "    y = torch.tensor(self.y_frame[index], dtype=torch.float32)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YxR8TknxJ2Io"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QF1Z099wJ2Gb"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wOWyu19_J2EZ"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MJTZAUbQJ2Ca"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp((torch.arange(0, embed_dim, 2)) * (-math.log(10000.0) / embed_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim=512, num_heads=8, num_layers=6, ff_dim=2048, dropout=0.1, vocab_size=10000, max_len=5000):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_encoding = PositionalEncoding(embed_dim=embed_dim, max_len=max_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer=encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.y_labels_out = nn.Linear(embed_dim, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.position_encoding(x)\n",
        "\n",
        "        x = self.encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        y_label_out = self.y_labels_out(x)\n",
        "        return y_label_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3y4bHJVbJ2AQ"
      },
      "outputs": [],
      "source": [
        "model = Transformer(\n",
        "    embed_dim=hyperparameters['embed_dim'],\n",
        "    num_heads=hyperparameters['num_heads'],\n",
        "    num_layers=hyperparameters['num_layers'],\n",
        "    ff_dim=hyperparameters['ff_dim'],\n",
        "    dropout=hyperparameters['dropout'],\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q00nuF8HJ1-H",
        "outputId": "7161041c-b838-46e6-9980-c0ea441ca372"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embeddings): Embedding(66, 128)\n",
              "  (position_encoding): PositionalEncoding()\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (y_labels_out): Linear(in_features=128, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2Qa9JHAKI1-",
        "outputId": "eeca18f2-2b6e-4d9f-e103-3a1c50e52814"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_4lFoaqKIz4",
        "outputId": "56908cc9-b023-4524-d38e-2d3a2965a735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 668805\n"
          ]
        }
      ],
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tDYvfOC7KIxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce80487d-0816-49c7-d066-bf14d8f105ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2655993476.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'], weight_decay=1e-4)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3YrLO_deKIvu"
      },
      "outputs": [],
      "source": [
        "def train32(model, loader, ce, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device).long()\n",
        "\n",
        "        output = model(x)\n",
        "        loss = ce(output, y)\n",
        "\n",
        "        prediction = torch.argmax(output, dim=1)\n",
        "        correct += (prediction == y).sum().item()\n",
        "        total += len(x)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * len(x)\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return (\n",
        "        running_loss / len(loader.dataset),\n",
        "        accuracy\n",
        "    )\n",
        "\n",
        "def train16(model, loader, ce, optimizer, scaler):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        optimizer.zero_grad()\n",
        "        x = x.to(device)\n",
        "        y = y.to(device).long()\n",
        "\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "            output = model(x)\n",
        "            loss = ce(output, y)\n",
        "\n",
        "        prediction = torch.argmax(output, dim=1)\n",
        "        correct += (prediction == y).sum().item()\n",
        "        total += len(x)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * len(x)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return (\n",
        "        running_loss / len(loader.dataset),\n",
        "        accuracy\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hK6At-_VKItX"
      },
      "outputs": [],
      "source": [
        "def validation(model, loader, ce):\n",
        "    model.eval()\n",
        "\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).long()\n",
        "\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                output = model(x)\n",
        "                loss = ce(output, y)\n",
        "\n",
        "            running_loss += loss.item() * len(x)\n",
        "\n",
        "            prediction = torch.argmax(output, dim=1)\n",
        "            correct += (prediction == y).sum().item()\n",
        "\n",
        "            total += len(x)\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return (\n",
        "        running_loss / len(loader.dataset),\n",
        "        accuracy\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mL_srk1KQNu",
        "outputId": "952940a1-2d11-4d7c-e21e-bf779d803097"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2224334287.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
            "/tmp/ipython-input-601143261.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch (1/50): Train Loss = 1.0305, Valitation Loss = 1.0313, Train_acc = 0.5740, Val_acc = 0.5812\n",
            "Epoch (2/50): Train Loss = 1.0208, Valitation Loss = 1.0325, Train_acc = 0.5770, Val_acc = 0.5812\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (3/50): Train Loss = 1.0170, Valitation Loss = 1.0325, Train_acc = 0.5768, Val_acc = 0.5812\n",
            "No improvement in val loss. Counter = 2/10\n",
            "Epoch (4/50): Train Loss = 1.0145, Valitation Loss = 1.0242, Train_acc = 0.5773, Val_acc = 0.5823\n",
            "Epoch (5/50): Train Loss = 1.0107, Valitation Loss = 1.0297, Train_acc = 0.5791, Val_acc = 0.5847\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (6/50): Train Loss = 1.0092, Valitation Loss = 1.0236, Train_acc = 0.5800, Val_acc = 0.5845\n",
            "Epoch (7/50): Train Loss = 1.0061, Valitation Loss = 1.0192, Train_acc = 0.5817, Val_acc = 0.5853\n",
            "Epoch (8/50): Train Loss = 1.0042, Valitation Loss = 1.0201, Train_acc = 0.5844, Val_acc = 0.5844\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (9/50): Train Loss = 1.0027, Valitation Loss = 1.0182, Train_acc = 0.5853, Val_acc = 0.5873\n",
            "Epoch (10/50): Train Loss = 1.0008, Valitation Loss = 1.0192, Train_acc = 0.5863, Val_acc = 0.5867\n",
            "Model saved at /content/drive/MyDrive/dna-mulation-50K/model_Third_Run_epoch_10.pth\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (11/50): Train Loss = 0.9994, Valitation Loss = 1.0196, Train_acc = 0.5889, Val_acc = 0.5851\n",
            "No improvement in val loss. Counter = 2/10\n",
            "Epoch (12/50): Train Loss = 0.9973, Valitation Loss = 1.0227, Train_acc = 0.5897, Val_acc = 0.5791\n",
            "No improvement in val loss. Counter = 3/10\n",
            "Epoch (13/50): Train Loss = 0.9948, Valitation Loss = 1.0157, Train_acc = 0.5905, Val_acc = 0.5841\n",
            "Epoch (14/50): Train Loss = 0.9937, Valitation Loss = 1.0169, Train_acc = 0.5905, Val_acc = 0.5808\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (15/50): Train Loss = 0.9906, Valitation Loss = 1.0117, Train_acc = 0.5908, Val_acc = 0.5938\n",
            "Epoch (16/50): Train Loss = 0.9887, Valitation Loss = 1.0089, Train_acc = 0.5898, Val_acc = 0.5875\n",
            "Epoch (17/50): Train Loss = 0.9868, Valitation Loss = 1.0069, Train_acc = 0.5917, Val_acc = 0.5942\n",
            "Epoch (18/50): Train Loss = 0.9839, Valitation Loss = 1.0074, Train_acc = 0.5932, Val_acc = 0.5915\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (19/50): Train Loss = 0.9821, Valitation Loss = 1.0067, Train_acc = 0.5946, Val_acc = 0.5917\n",
            "Epoch (20/50): Train Loss = 0.9808, Valitation Loss = 1.0062, Train_acc = 0.5938, Val_acc = 0.5924\n",
            "Model saved at /content/drive/MyDrive/dna-mulation-50K/model_Third_Run_epoch_20.pth\n",
            "Epoch (21/50): Train Loss = 0.9800, Valitation Loss = 1.0035, Train_acc = 0.5927, Val_acc = 0.5932\n",
            "Epoch (22/50): Train Loss = 0.9782, Valitation Loss = 1.0061, Train_acc = 0.5926, Val_acc = 0.5885\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (23/50): Train Loss = 0.9770, Valitation Loss = 1.0029, Train_acc = 0.5951, Val_acc = 0.5925\n",
            "Epoch (24/50): Train Loss = 0.9777, Valitation Loss = 1.0121, Train_acc = 0.5944, Val_acc = 0.5905\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (25/50): Train Loss = 0.9755, Valitation Loss = 1.0023, Train_acc = 0.5950, Val_acc = 0.5950\n",
            "Epoch (26/50): Train Loss = 0.9747, Valitation Loss = 1.0019, Train_acc = 0.5948, Val_acc = 0.5902\n",
            "Epoch (27/50): Train Loss = 0.9734, Valitation Loss = 1.0056, Train_acc = 0.5963, Val_acc = 0.5935\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (28/50): Train Loss = 0.9725, Valitation Loss = 1.0053, Train_acc = 0.5967, Val_acc = 0.5934\n",
            "No improvement in val loss. Counter = 2/10\n",
            "Epoch (29/50): Train Loss = 0.9712, Valitation Loss = 1.0018, Train_acc = 0.5970, Val_acc = 0.5917\n",
            "Epoch (30/50): Train Loss = 0.9710, Valitation Loss = 1.0032, Train_acc = 0.5963, Val_acc = 0.5934\n",
            "Model saved at /content/drive/MyDrive/dna-mulation-50K/model_Third_Run_epoch_30.pth\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (31/50): Train Loss = 0.9703, Valitation Loss = 0.9986, Train_acc = 0.5975, Val_acc = 0.5904\n",
            "Epoch (32/50): Train Loss = 0.9695, Valitation Loss = 1.0032, Train_acc = 0.5979, Val_acc = 0.5947\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (33/50): Train Loss = 0.9687, Valitation Loss = 0.9992, Train_acc = 0.5968, Val_acc = 0.5974\n",
            "No improvement in val loss. Counter = 2/10\n",
            "Epoch (34/50): Train Loss = 0.9682, Valitation Loss = 1.0032, Train_acc = 0.5973, Val_acc = 0.5925\n",
            "No improvement in val loss. Counter = 3/10\n",
            "Epoch (35/50): Train Loss = 0.9668, Valitation Loss = 0.9991, Train_acc = 0.5974, Val_acc = 0.5921\n",
            "No improvement in val loss. Counter = 4/10\n",
            "Epoch (36/50): Train Loss = 0.9660, Valitation Loss = 0.9992, Train_acc = 0.5995, Val_acc = 0.5885\n",
            "No improvement in val loss. Counter = 5/10\n",
            "Epoch (37/50): Train Loss = 0.9645, Valitation Loss = 0.9983, Train_acc = 0.5992, Val_acc = 0.5896\n",
            "Epoch (38/50): Train Loss = 0.9647, Valitation Loss = 1.0022, Train_acc = 0.5985, Val_acc = 0.5951\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (39/50): Train Loss = 0.9644, Valitation Loss = 0.9987, Train_acc = 0.5997, Val_acc = 0.5918\n",
            "No improvement in val loss. Counter = 2/10\n",
            "Epoch (40/50): Train Loss = 0.9628, Valitation Loss = 0.9988, Train_acc = 0.5998, Val_acc = 0.5919\n",
            "Model saved at /content/drive/MyDrive/dna-mulation-50K/model_Third_Run_epoch_40.pth\n",
            "No improvement in val loss. Counter = 3/10\n",
            "Epoch (41/50): Train Loss = 0.9620, Valitation Loss = 0.9951, Train_acc = 0.6010, Val_acc = 0.5956\n",
            "Epoch (42/50): Train Loss = 0.9609, Valitation Loss = 1.0076, Train_acc = 0.6016, Val_acc = 0.5846\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (43/50): Train Loss = 0.9597, Valitation Loss = 0.9986, Train_acc = 0.6015, Val_acc = 0.5957\n",
            "No improvement in val loss. Counter = 2/10\n",
            "Epoch (44/50): Train Loss = 0.9597, Valitation Loss = 0.9958, Train_acc = 0.6034, Val_acc = 0.5939\n",
            "No improvement in val loss. Counter = 3/10\n",
            "Epoch (45/50): Train Loss = 0.9579, Valitation Loss = 0.9955, Train_acc = 0.6026, Val_acc = 0.5964\n",
            "No improvement in val loss. Counter = 4/10\n",
            "Epoch (46/50): Train Loss = 0.9569, Valitation Loss = 0.9974, Train_acc = 0.6017, Val_acc = 0.5975\n",
            "No improvement in val loss. Counter = 5/10\n",
            "Epoch (47/50): Train Loss = 0.9572, Valitation Loss = 0.9941, Train_acc = 0.6016, Val_acc = 0.5974\n",
            "Epoch (48/50): Train Loss = 0.9559, Valitation Loss = 0.9942, Train_acc = 0.6038, Val_acc = 0.5957\n",
            "No improvement in val loss. Counter = 1/10\n",
            "Epoch (49/50): Train Loss = 0.9549, Valitation Loss = 0.9996, Train_acc = 0.6048, Val_acc = 0.5872\n",
            "No improvement in val loss. Counter = 2/10\n",
            "Epoch (50/50): Train Loss = 0.9545, Valitation Loss = 0.9981, Train_acc = 0.6029, Val_acc = 0.5979\n",
            "Model saved at /content/drive/MyDrive/dna-mulation-50K/model_Third_Run_epoch_50.pth\n",
            "No improvement in val loss. Counter = 3/10\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "early_stop = False\n",
        "\n",
        "training_loss_arr = []\n",
        "validation_loss_arr = []\n",
        "accuracy_arr = []\n",
        "\n",
        "save_dir = f\"/content/drive/MyDrive/dna-mulation-{hyperparameters['dataset_size']}\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(hyperparameters['epochs']):\n",
        "    if hyperparameters['Precision'] == 'FP32':\n",
        "        train_loss, train_acc = train32(\n",
        "            model,\n",
        "            train_loader,\n",
        "            ce,\n",
        "            optimizer\n",
        "        )\n",
        "    else:\n",
        "        train_loss, train_acc = train16(\n",
        "            model,\n",
        "            train_loader,\n",
        "            ce,\n",
        "            optimizer,\n",
        "            scaler\n",
        "        )\n",
        "\n",
        "    val_loss, val_acc = validation(\n",
        "        model,\n",
        "        test_loader,\n",
        "        ce\n",
        "    )\n",
        "\n",
        "    print(f\"Epoch ({epoch+1}/{50}): Train Loss = {train_loss:.4f}, Valitation Loss = {val_loss:.4f}, Train_acc = {train_acc:.4f}, Val_acc = {val_acc:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      checkpoint_path = f\"{save_dir}/model_{hyperparameters['name']}_epoch_{epoch+1}.pth\"\n",
        "      torch.save({\n",
        "          'epoch': epoch+1,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'train_losses': train_loss,\n",
        "          'val_losses': val_loss\n",
        "      }, checkpoint_path)\n",
        "      print(f\"Model saved at {checkpoint_path}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        continue\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"No improvement in val loss. Counter = {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            early_stop = True\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTgNsv_WKSKt",
        "outputId": "c5706acd-c9aa-4abd-fa99-7c32ad187588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Label:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.57      0.01      0.03       616\n",
            "         1.0       0.54      0.11      0.18      3081\n",
            "         2.0       0.60      0.97      0.74      5812\n",
            "         3.0       0.53      0.04      0.08       235\n",
            "         4.0       0.00      0.00      0.00       256\n",
            "\n",
            "    accuracy                           0.60     10000\n",
            "   macro avg       0.45      0.23      0.21     10000\n",
            "weighted avg       0.57      0.60      0.49     10000\n",
            "\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def get_predictions_and_labels(model, loader):\n",
        "    model.eval()\n",
        "    all_y_true = []\n",
        "\n",
        "    all_y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            output = model(x)\n",
        "\n",
        "            _, prediction = torch.max(output, 1)\n",
        "\n",
        "            all_y_true.extend(y.cpu().numpy())\n",
        "\n",
        "            all_y_pred.extend(prediction.cpu().numpy())\n",
        "\n",
        "    return (all_y_true, all_y_pred)\n",
        "\n",
        "y_true, y_pred = get_predictions_and_labels(model, test_loader)\n",
        "\n",
        "print(\"Classification Report for Label:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(\"-\"*20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-5hlNhuRa8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}